# transformers-examples-ja

ã«ã»ã‚“ã”ã®  
ã°ãƒ¼ã¨ã®  
ã¹ã‚“ãã‚‡ã†

## ãƒªãƒ³ã‚¯
- [huggingface/transformers: ğŸ¤—Transformers: State-of-the-art Natural Language Processing for Pytorch and TensorFlow 2.0.](https://github.com/huggingface/transformers)
- [cl-tohoku/bert-japanese: BERT models for Japanese text.](https://github.com/cl-tohoku/bert-japanese)
- [himkt/awesome-bert-japanese: ğŸ“ A list of pre-trained BERT models for Japanese with word/subword tokenization + vocabulary construction algorithm information](https://github.com/himkt/awesome-bert-japanese)

## bert-search

BERTã®æ–‡ç« åŸ‹ã‚è¾¼ã¿ã‚’ç”¨ã„ãŸé¡ä¼¼æ–‡æ›¸æ¤œç´¢ã‚¢ãƒ—ãƒªã€‚

```shell
# Run server
docker-compose -f docker/docker-compose.yaml up
# Execute search
curl "http://localhost:8000/api/search" --get --data-urlencode "q=é€æ–™ã¯ãªã‚“ã¼ã§ã™ã‹"
```

## sentence_embedding

äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¦æ–‡ç« ã®åŸ‹ã‚è¾¼ã¿è¡¨ç¾ã‚’å¾—ã‚‹ã€‚

`BertModel.forward`ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`last_hidden_state`ã¨`pooler_output`ã‚’è¿”ã™ã€‚ãã‚Œãã‚Œã®æ¬¡å…ƒã¯`(batch, seq_len, embedding_dim)` ã¨`(batch, embedding_dim)`ã§ã‚ã‚Šã€`pooler_output`ã¯`lasy_hidden_state`ã®`[CLS]`ãƒˆãƒ¼ã‚¯ãƒ³ã«ä¸€å›linerã‚’é€šã—ãŸã ã‘ã®ã‚‚ã®ã€‚maxã‚„avgã§poolingã—ãŸã„å ´åˆã¯ã‚³ãƒ¼ãƒ‰ä¾‹ã®ã‚ˆã†ã«å‹æ‰‹ã«ã‚„ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚


```shell
$ poetry run python src/sentence_embedding.py
result[0].size():  torch.Size([1, 21, 768])
result[1].size():  torch.Size([1, 768])
bert_pooled_embed.size():  torch.Size([768])
avg_pooled_embed.size() torch.Size([768])
max_pooled_embed.size() torch.Size([768])
concat_pooled_embed.size():  torch.Size([1536])
```

## mask_token_prediction

ãƒã‚¹ã‚¯ã•ã‚ŒãŸå˜èªãŒä½•ã‹ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§Bertã®äº‹å‰å­¦ç¿’ã«åˆ©ç”¨ã•ã‚Œã‚‹ãŸã‚ãã®ã¾ã¾åˆ©ç”¨ã§ãã‚‹ã€‚

```shell
$ poetry run python src/mask_token_prediction.py

Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[CLS] å±±ç”° ã•ã‚“ ãŒ ã‚´ã‚¸ãƒ© ã‚’ è¦‹ ãŸ ã® ã¯ ã“ã‚Œ ãŒ åˆã‚ã¦ ã§ã— ãŸ ã€‚ å·¨å¤§ ã ã£ ãŸ ã€‚ [SEP]
[CLS] å±±ç”° ã•ã‚“ ãŒ æ˜ ç”» ã‚’ è¦‹ ãŸ ã® ã¯ ã“ã‚Œ ãŒ åˆã‚ã¦ ã§ã— ãŸ ã€‚ å·¨å¤§ ã ã£ ãŸ ã€‚ [SEP]
[CLS] å±±ç”° ã•ã‚“ ãŒ ã“ã‚Œ ã‚’ è¦‹ ãŸ ã® ã¯ ã“ã‚Œ ãŒ åˆã‚ã¦ ã§ã— ãŸ ã€‚ å·¨å¤§ ã ã£ ãŸ ã€‚ [SEP]
[CLS] å±±ç”° ã•ã‚“ ãŒ ãã‚Œ ã‚’ è¦‹ ãŸ ã® ã¯ ã“ã‚Œ ãŒ åˆã‚ã¦ ã§ã— ãŸ ã€‚ å·¨å¤§ ã ã£ ãŸ ã€‚ [SEP]
[CLS] å±±ç”° ã•ã‚“ ãŒ å¯Œå£«å±± ã‚’ è¦‹ ãŸ ã® ã¯ ã“ã‚Œ ãŒ åˆã‚ã¦ ã§ã— ãŸ ã€‚ å·¨å¤§ ã ã£ ãŸ ã€‚ [SEP]
```
